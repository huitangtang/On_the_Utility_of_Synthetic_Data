<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->
    <style type="text/css">
        @font-face {
            font-family: 'Avenir Book';
            src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
        }
    body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 900px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>
	<title>A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation</title>
</head>

<body>
<br>
<span style="font-size:36px">
    <div style="text-align: center;">
        A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation
    </div>
</span>
<br>
<br>
<br>
<table align="center" width="700px">
    <tr>
        <td align="center" width="300px">
            <div style="text-align: center;">
                <span style="font-size:16px"><a href="https://huitangtang.github.io/">Hui Tang</a><sup>1, 2</sup></span>
            </div>
        </td>
        
        <td align="center" width="300px">
            <div style="text-align: center;">
                <span style="font-size:16px">
                    <a href="http://kuijia.site/">Kui Jia</a>
<!--                    <sup><img class="round" style="width:20px" src="./resources/corresponding_fig.png">3</sup>-->
                    <sup>&#9993, 1</sup>
                </span>
            </div>
        </td>
    </tr>
</table>

<br>
	
<table align="center" width="700px">
    <tbody>
    <tr>
        <td align="center" width="300px">
            <center>
                <span style="font-size:16px">South China University of Technology<sup>1</sup></span>
            </center>
        </td>

        <td align="center" width="300px">
            <center>
                <span style="font-size:16px">DexForce Co. Ltd.<sup>2</sup></span>
            </center>
        </td>

    </tr>
    </tbody>
</table>


<table align="center" width="700px">
    <tbody>
    <tr>
        <td align="center" width="300px">
            <center>
                <span style="font-size:16px"><sup>&#9993</sup>Corresponding author</span>
            </center>
        </td>
    </tr>
    </tbody>
</table>

<table align="center" width="900px">
    <tbody>
    <tr>
        <td align="center" width="200px">
            <div style="text-align: center;">
                <span style="font-size:20px">
                    Code
                    <a href="https://github.com/huitangtang/On_the_Utility_of_Synthetic_Data">[GitHub]</a>
                </span>
            </div>
        </td>
	    
	<td align="center" width="200px">
            <div style="text-align: center;">
                <span style="font-size:20px">
                    Dataset
                    <a href="https://cove.thecvf.com/datasets/892">[COVE]</a>
                </span>
            </div>
        </td>

        <td align="center" width="200px">
            <div style="text-align: center;">
                <span style="font-size:20px">
                    Paper
                    <a href="https://arxiv.org/abs/2303.09165">[arXiv]</a>
                </span>
            </div>
        </td>

        <td align="center" width="200px">
            <center>
                <span style="font-size:20px">
                    Cite <a href="resources/cite.txt">[BibTeX]</a>
                </span>
            </center>
        </td>
    </tr>
    </tbody>
</table>
<br>
<hr>

<div style="text-align: center;">
    <h2>Teaser</h2>
</div>


<p style="text-align:justify; text-justify:inter-ideograph;">
<table>
    <tr>
        <td>
            <div style="text-align: center;">
                <img src="resources/fig8.png" width="800px">
            </div>
        </td>
    </tr>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
            <!--  <i>i.e.</i>  <span style="color: red; "><b>red</b></span>  <span style="color: #1230F5; "><b>blue</b></span>  -->
		    Sample images from the synthetic (left) domain and the real domains of our introduced S2RDA-49 (middle) and S2RDA-MS-39 (right).
		    The real domain of S2RDA-49 comprises 60,535 images of 49 classes, collected from ImageNet validation set, ObjectNet, VisDA-2017 validation set, and the web. 
		    For S2RDA-MS-39, the real domain collects 41,735 natural images exclusive for 39 classes from MetaShift, which contain complex and distinct contexts, 
		    e.g., object presence (co-occurrence of different objects), general contexts (indoor or outdoor), and object attributes (color or shape), 
		    leading to a much harder task.
            </p>
        </td>
    </tr>
</table>


<br>
<hr>
<div style="text-align: center;">
    <h2>Abstract</h2>
</div>

<table>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
            To solve the basic and important problems in the context of image classification, such as the lack of comprehensive synthetic data research and the insufficient exploration of synthetic-to-real transfer, we in this paper propose to exploit synthetic datasets to explore questions on model generalization, benchmark pre-training strategies for domain adaptation (DA), and build a large-scale benchmark dataset S2RDA for synthetic-to-real transfer, which can push forward future DA research.
Specifically, we make the following contributions:
(i) under the well-controlled, IID data setting enabled by 3D rendering, we systematically verify the typical, important learning insights, e.g., shortcut learning, and discover the new laws of various data regimes and network architectures in generalization;
(ii) we further investigate the effect of image formation factors on generalization, e.g., object scale, material texture, illumination, camera viewpoint, and background in a 3D scene;
(iii) we use the simulation-to-reality adaptation as a downstream task for comparing the transferability between synthetic and real data when used for pre-training, which demonstrates that synthetic data pre-training is also promising to improve real test results;
finally, (iv) we develop a new large-scale synthetic-to-real benchmark for image classification, termed S2RDA, which provides more significant challenges for transfer from simulation to reality.
            </p>
        </td>
    </tr>
</table>

<br>
<hr>
<div style="text-align: center;">
    <h2>Background</h2>
</div>

<table>
    <tr>
        <td>
            <div style="text-align: center;">
                <img src="resources/fig1.png" width="500px">
            </div>
        </td>
    </tr>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                <br>
                Problem definition of unsupervised domain adaptation (UDA).
		The great success of deep learning relies on massive labeled data. Many tasks lack labeled data. How to address this problem?
		We can tackle it with data-efficient learning, such as transfer learning, UDA, etc. If the new task lacks high-quality training data, the knowledge from the previous task can be transferred to the new task.
		In this paper, we focus on the typical scenario of UDA — transfer knowledge of synthetic data to help classify real data.
<!-- <b>Left:</b> the two-step training procedure for finetuning the pre-trained CLIP to get CLIP-Attr that better aligns the regional visual feature to attributes. <b>Step-I:</b>  naive federate training by base attribute annotations. <b>Step-II:</b>  training by image-caption pairs. We first conduct RPN on the whole image to get box-level crops, parse the caption to get noun phrases, categories, and attributes, and then match these fine-grained concepts for weakly supervised training.
 <b>Right:</b> the proposed one-stage framework OvarNet. We inherit the CLIP-Attr for open-vocabulary object attribute recognition. Regional visual feature is learned from the attentional pooling of proposals; while attribute concept embedding is extracted from the text encoder. Solid lines declare the standard federated training regime. Dashed lines denote training by knowledge distillation with CLIP-Attr. -->
            </p>
        </td>
    </tr>
</table>
	
<br>
<hr>
<div style="text-align: center;">
    <h2>Data Synthesis via Domain Randomization</h2>
</div>

<table>
    <tr>
        <td>
            <div style="text-align: center;">
                <img src="resources/fig2.png" width="800px">
            </div>
        </td>
    </tr>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                <br>
                Sample images from the training (left) and validation (middle) domains of VisDA-2017 and our synthesized data (right).
		It is noteworthy that VisDA-2017 generates synthetic images by rendering 3D models just under varied camera angles and lighting conditions. 
		Differently, we vary the values of much more image variation factors, leading to more realistic and diverse samples.
            </p>
        </td>
    </tr>
</table>

<br>
<hr>
<div style="text-align: center;">
    <h2>Experiment and Evaluation</h2>
</div>

<div style="text-align: center;">
    <h3>Empirical Study on Supervised Learning</h3>
</div>

<table>
    <tr>
        <td>
            <p>
                <b>
                    R1: Fixed-Dataset Periodic Training vs. Training on Non-Repetitive Samples
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                With strong data augmentation, the test results on synthetic data without background are good enough to show that the synthetically trained models do not learn shortcut solutions relying on context clues.
            </p>
	    <br>
            <div style="text-align: center;">
                <img src="resources/tab1.png" width="900px">
            </div>
	    <p style="text-align: center;">
		Training on a fixed dataset vs. non-repetitive samples. FD: Fixed Dataset, True (T) or False (F). DA: Data Augmentation, None (N), Weak (W), or Strong (S). BG: BackGround.
	    </p>
	    <br>
	    <div style="text-align: center;">
                <img src="resources/fig3.png" width="800px">
            </div>
	    <p style="text-align: center;">
		 Learning process. (a-c): Training ResNet-50 on a fixed dataset (<span style="color: blue; "><b>blue</b></span>) or non-repetitive samples (<span style="color: red; "><b>red</b></span>) for no, weak, and strong data augmentations. 
		 (d): Training ResNet-50 (<span style="color: red; "><b>red</b></span>), ViT-B (<span style="color: green; "><b>green</b></span>), and Mixer-B (<span style="color: blue; "><b>blue</b></span>) on non-repetitive samples with strong data augmentation.
	    </p>
	    <br>
	    <div style="text-align: center;">
                <img src="resources/fig4.png" width="800px">
            </div>
	    <p style="text-align: center;">
		 Attention maps of randomly selected IID test samples, obtained from the ViT-B trained on a fixed dataset or non-repetitive samples with no data augmentation, at the 20-th, 200-th, 2K-th, 20K-th, and 200K-th training iterations.
	    </p>
        </td>
    </tr>


    <tr>
        <td>
            <br>
            <p>
                <b>
                    R2: Evaluating Various Network Architectures
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                In IID tests, ViT performs surprisingly poorly whatever the data augmentation is and even the triple number of training epochs does not improve much.
            </p>
        </td>
    </tr>

    <tr>
        <td>
            <br>
            <p>
                <b>
                    R3: Impact of Model Capacity & Impact of Training Data Quantity
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                There is always a bottleneck from synthetic data to OOD/real data, where increasing data size and model capacity brings no more benefits, and DA to bridge the distribution gap is indispensable except for evolving the image generation pipeline to synthesize more realistic images.
            </p>
	    <br>
            <div style="text-align: center;">
                <img src="resources/fig5.png" width="500px">
            </div>
	    <p style="text-align: center;">
		 Generalization accuracy w.r.t. model capacity.
	    </p>
	    <br>
	    <div style="text-align: center;">
                <img src="resources/fig6.png" width="500px">
            </div>
	    <p style="text-align: center;">
		 Generalization accuracy w.r.t. training data quantity.
	    </p>
        </td>
    </tr>
	
    <tr>
        <td>
            <br>
            <p>
                <b>
                    R4: Impact of Data Augmentations
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                For the data-unrepeatable training, IID and OOD generalizations are some type of zero-sum game w.r.t. the strength of data augmentation.
            </p>
        </td>
    </tr>

</table>

<br>
<hr>
<div style="text-align: center;">
    <h3>Assessing Image Variation Factors</h3>
</div>

<table>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
		In this table, we explore how variation factors of an image affect the model generalization, 
		such as object scale, material texture, illumination, camera viewpoint, and background. 
		We find that different rendering variation factors and even their different values have uneven importance to model generalization. 
		The observations also stress the under-explored topic of data generation — AutoSimulate, namely Weighted Rendering, 
		which learns distributions of image variation factors from real/target data.
            </p>
	    <br>
            <div style="text-align: center;">
                <img src="resources/tab2.png" width="500px">
            </div>
	    <p style="text-align: center;">
		 Fix vs. randomize image variation factors (ResNet-50).
	    </p>
        </td>
    </tr>
</table>

<br>
<hr>
<div style="text-align: center;">
    <h3>Exploring Pre-training for Domain Adaptation</h3>
</div>

<table>
    <tr>
        <td>
            <p>
                <b>
                    R1: The Importance of Pre-training for DA
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                DA fails without pre-training. With no pre-training, the very baseline No Adaptation that trains the model only on the labeled source data, 
		outperforms all compared DA methods in overall accuracy, despite the worst mean class precision.
            </p>
	    <br>
            <div style="text-align: center;">
                <img src="resources/tab3.png" width="900px">
            </div>
	    <p style="text-align: center;">
		Comparing different pre-training schemes. 
		⋆: Official checkpoint. Green or red: Best Acc. or Mean in each row. Ours w. SelfSup: Sup. pre-training with contrastive learning.
	    </p>
        </td>
    </tr>

    <tr>
        <td>
	    <br>
            <p>
                <b>
                    R2: Effects of Different Pre-training Schemes
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                Different DA methods exhibit different relative advantages under different pre-training data. 
		When pre-training on our synthesized data, MCD achieves the best results; 
		when pre-training on Ours+SubImageNet, DisClusterDA outperforms the others;
		when pre-training on ImageNet⋆, SRDC yields the best performance. 
		What’s worse, the reliability of existing DA method evaluation criteria is unguaranteed. 
		With different pre-training schemes, the best performance is achieved by different DA methods.
            </p>
        </td>
    </tr>
    
    <tr>
        <td>
	    <br>
            <p>
                <b>
                    R3: Synthetic Data Pre-training vs. Real Data Pre-training
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                Synthetic data pre-training is comparable to or better than real data pre-training — Synthetic data pretraining is promising. 
		Under the same experimental configuration, SynSL pre-training for 24 epochs is comparable to or better than pre-training on ImageNet for 120 epochs.
            </p>
	    <br>
            <div style="text-align: center;">
                <img src="resources/fig7.png" width="900px">
            </div>
	    <p style="text-align: center;">
		Learning process (Mean) of MCD (left) and DisClusterDA (right) when varying the pre-training scheme.
	    </p>
        </td>
    </tr>

    <tr>
        <td>
	    <br>
            <p>
                <b>
                    R4: Implications for Pre-training Data Setting
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                Big Synthesis Small Real is worth deeply researching. Ours+SubImageNet augmenting our synthetic data with a small amount of real data, 
		achieves remarkable performance gain over Ours, suggesting a promising paradigm of supervised pre-training — Big Synthesis Small Real. 
		On the other hand, pre-train with target classes first under limited computing resources. With 200K pre-training iterations, 
		SubImageNet performs much better than ImageNet (10 Epochs).
            </p>
        </td>
    </tr>
	
    <tr>
        <td>
	    <br>
            <p>
                <b>
                    R5: The Improved Generalization of DA Models
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                Real data pre-training with extra non-target classes, fine-grained target subclasses, or our synthesized data added for target classes helps DA. 
		ImageNet (120 Epochs) involving both target and non-target classes in pre-training is better than SubImageNet involving only target classes, 
		indicating that learning rich category relationships is helpful for downstream transferring. 
		with 200K pre-training iterations, ImageNet-990 performs much worse than ImageNet, implying that pre-training in a fine-grained visual categorization manner may bring surprising benefits. 
		Ours+SubImageNet adding our synthesized data for target classes in SubImageNet, produces significant improvements and is close to ImageNet (120 Epochs); 
		ImageNet-990+Ours improves over ImageNet-990, suggesting that synthetic data may help improve the performance further.
            </p>
        </td>
    </tr>
	
    <tr>
        <td>
	    <br>
            <p>
                <b>
                    R6: Convergence Analysis
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                The convergence from different pre-training schemes for the same DA method differs in speed, stability, and accuracy.
		SynSL with 24 epochs outperforms ImageNet with 120 epochs significantly; notably, SynSL is on par with or better than ImageNet⋆.
            </p>
        </td>
    </tr>
</table>

<br>
<hr>
<div style="text-align: center;">
    <h3> A New Synthetic-to-Real Benchmark</h3>
</div>

<table>
    <tr>
        <td>
            <p>
                <b>
                    R1: Dataset Construction
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
		Our proposed Synthetic-to-Real benchmark for more practical visual DA (termed S2RDA) includes two challenging transfer tasks of S2RDA-49 and S2RDA-MS-39.
		In each task, source/synthetic domain samples are synthesized by rendering 3D models from ShapeNet. 
		The used 3D models are in the same label space as the target/real domain and each class has 12K rendered RGB images. 
		The real domain of S2RDA-49 comprises 60,535 images of 49 classes, collected from ImageNet validation set, ObjectNet, VisDA-2017 validation set, and the web. 
		For S2RDA-MS-39, the real domain collects 41,735 natural images exclusive for 39 classes from MetaShift, which contain complex and distinct contexts, 
		e.g., object presence (co-occurrence of different objects), general contexts (indoor or outdoor), and object attributes (color or shape), 
		leading to a much harder task. 
		Compared to VisDA-2017, our introduced S2RDA contains more categories, 
		more realistically synthesized source domain data coming for free, 
		and more complicated target domain data collected from diverse real-world sources, 
		setting a more practical and challenging benchmark for future DA research.
            </p>
	    <br>
            <div style="text-align: center;">
                <img src="resources/fig9.png" width="800px">
            </div>
	    <p style="text-align: center;">
		 The distribution of the number of images per class in each real domain, exhibited to be a long-tailed distribution where a small number of classes dominate.
	    </p>
        </td>
    </tr>
	
    <tr>
        <td>
	    <br>
            <p>
                <b>
                    R2: Benchmarking DA Methods
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
		We report the results on S2RDA in the following table and show t-SNE visualizations in the following figure. 
		SRDC outperforms the baseline No Adaptation by ∼10% on S2RDA-49 and DisClusterDA outperforms that by ∼5%, verifying the efficacy of these DA methods. 
		The observations also demonstrate that S2RDA can benchmark different DA methods. Compared to SubVisDA-10, SRDC degrades by ∼7% on S2RDA-49, which is reasonable 
		as our real domain contains more practical images from real-world sources, though our synthetic data contain much more diversity, e.g., background. 
		Differently, S2RDA-MS-39, which decreases by >20% over S2RDA-49, evaluates different DA approaches on the worst/extreme cases, making a more comprehensive comparison
		and acting as a touchstone to examine and advance DA algorithms. Reducing domain gap between simple and difficult backgrounds is by nature one of the key issues in simulation-to-real transfer. 
		To sum up, S2RDA is a better benchmark than VisDA-2017 and enables a larger room of improvement.
            </p>
	    <br>
            <div style="text-align: center;">
                <img src="resources/tab4.png" width="800px">
            </div>
	    <p style="text-align: center;">
		 Domain adaptation performance on S2RDA (ResNet-50).
	    </p>
	    <br>
            <div style="text-align: center;">
                <img src="resources/fig10.png" width="800px">
            </div>
	    <p style="text-align: center;">
		 The t-SNE visualization of target domain features extracted by different models on S2RDA-49 (a-b) and S2RDA-MS-39 (c-d).
	    </p>
        </td>
    </tr>
</table>

	
<br>
<hr>

<div style="text-align: center;">
    <h2>BibTeX</h2>
</div>
      <pre>
  	<code>
    @InProceedings{tang2023a,
    author = {Tang, Hui and Jia, Kui},
    title = {A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2023}}
  	</code>
      </pre>
	
<br>
<hr>

<div style="text-align: center;">
    <h2>Acknowledgements</h2>
</div>
      <p>
	      Based on a template by <a href="https://kyanchen.github.io/OvarNet/">Keyan Chen</a>.
      </p>

<br>
<br>
<br>

</body>
</html>
